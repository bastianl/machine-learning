{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: This seems like a classification task, as we are are predicting a binary output (pass or fail) as opposed to a continuous output, for which regression would be more suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print(\"Student data read successfully!\")\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = len(student_data)\n",
    "n_features = len(student_data.columns) - 1  # exclude 'passed' column\n",
    "n_passed = np.sum(student_data['passed'] == 'yes')\n",
    "n_failed = np.sum(student_data['passed'] == 'no')\n",
    "grad_rate = n_passed / (n_passed + n_failed) * 100  # python 3 uses \"true division\"\n",
    "print(\"Total number of students: {}\".format(n_students))\n",
    "print(\"Number of students who passed: {}\".format(n_passed))\n",
    "print(\"Number of students who failed: {}\".format(n_failed))\n",
    "print(\"Number of features: {}\".format(n_features))\n",
    "print(\"Graduation rate of the class: {:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print(\"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print(\"Target column: {}\".format(target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (49):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'passed']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "# Note: this preprocesses all columns include the \"target\" column which needs\n",
    "# to be converted from [\"yes\", \"no\"] to [1, 0]\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "student_data = preprocess_features(student_data)\n",
    "print(\"Processed feature columns ({}):-\\n{}\".format(len(student_data.columns), list(student_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature values:-\n",
      "   school_GP  school_MS  sex_F  sex_M  age  address_R  address_U  famsize_GT3  \\\n",
      "0          1          0      1      0   18          0          1            1   \n",
      "1          1          0      1      0   17          0          1            1   \n",
      "2          1          0      1      0   15          0          1            0   \n",
      "3          1          0      1      0   15          0          1            1   \n",
      "4          1          0      1      0   16          0          1            1   \n",
      "\n",
      "   famsize_LE3  Pstatus_A    ...     higher  internet  romantic  famrel  \\\n",
      "0            0          1    ...          1         0         0       4   \n",
      "1            0          0    ...          1         1         0       5   \n",
      "2            1          0    ...          1         1         0       4   \n",
      "3            0          0    ...          1         1         1       3   \n",
      "4            0          0    ...          1         0         0       4   \n",
      "\n",
      "   freetime  goout  Dalc  Walc  health  absences  \n",
      "0         3      4     1     1       3         6  \n",
      "1         3      3     1     1       3         4  \n",
      "2         3      2     2     3       3        10  \n",
      "3         2      2     1     1       5         2  \n",
      "4         3      2     1     2       5         4  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "X_all = student_data.drop([target_col], axis=1)  # feature values for all students\n",
    "print(\"\\nFeature values:-\")\n",
    "print(X_all.head())  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=(num_train / num_all), random_state=99)\n",
    "\n",
    "print(\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print(\"Test set: {} samples\".format(X_test.shape[0]))\n",
    "# Note: If you need a validation set, extract bit from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What is the theoretical O(n) time & space complexity in terms of input size?\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005196094512939453"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    return (end - start)\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for training set: 0.8108108108108109\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target, metric_func=f1_score):\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    return metric_func(target.values, y_pred), (end - start)\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print(\"F1 score for training set: {}\".format(train_f1_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8029197080291971\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print(\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    train_time = train_classifier(clf, X_train, y_train)\n",
    "    f1_train, prediction_time_train = predict_labels(clf, X_train, y_train)\n",
    "    f1_test, prediction_time_test, = predict_labels(clf, X_test, y_test)\n",
    "    return f1_train, f1_test, train_time, np.mean([prediction_time_train, prediction_time_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(clf, training_sizes=(100, 200, len(X_train))):\n",
    "    \"\"\"Evaluate and train a classifier. reports f1 score on train and test data,\n",
    "    as well as training and prediction time for various training sizes.\n",
    "    Uses the student data defined in the global namespace.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for k in training_sizes:\n",
    "        # data has already been shuffled, so we just use first k samples\n",
    "        f1_train, f1_test, train_time, prediction_time = train_predict(clf, X_train[:k], y_train[:k], X_test, y_test)\n",
    "        scores.append([k, f1_train, f1_test, train_time, prediction_time])\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['train_size', 'f1_train', 'f1_test', 'train_time', 'prediction_time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Max Depth = None (all features used)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621849</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train   f1_test  train_time  prediction_time\n",
       "0         100         1  0.776119    0.001009         0.000226\n",
       "1         200         1  0.715447    0.001204         0.000200\n",
       "2         300         1  0.621849    0.001928         0.000338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Max Depth = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.816568</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.831746</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.812030</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train   f1_test  train_time  prediction_time\n",
       "0         100  0.816568  0.774194    0.000693         0.000186\n",
       "1         200  0.831746  0.802721    0.000426         0.000226\n",
       "2         300  0.810811  0.812030    0.000714         0.000241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['failures']\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Max Depth = None (all features used)\")\n",
    "display(train_and_evaluate_classifier(clf))\n",
    "\n",
    "# Note, the base decision tree class overfits drastically, so we also show\n",
    "# performance of a simple decision tree stump.\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "print(\"-\" * 40)\n",
    "print(\"Max Depth = 1\")\n",
    "display(train_and_evaluate_classifier(clf))\n",
    "print([x for x in X_all.columns[clf.feature_importances_ != 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the theoretical O(n) time & space complexity in terms of input size?\n",
    "> The sklearn implementation of a Decision Tree has a time complexity of [$O(D * n * log(n))$](http://scikit-learn.org/stable/modules/tree.html#complexity) where `D` is the number of features, and `n` is the number of samples. An $O(n * log(n))$ running time in the number of samples is relatively fast. Prediction times are very fast, taking `O(max_depth)`. Space complexity grows with the complexity of our tree, but will not exceed `D`. It can be limited with `max_depth`, as well as other parameters.\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "> Decision trees are simple but robust classifiers. They can be easily visualized, and due to their simplicity they are very interpretable. Judging by our results above, they seems to perform reasonably well on small datasets. However, decision trees can also be prone to overfitting, which is why hyperparameter tuning is especially important. In this case we set `max_depth=2` to keep the implementation simple, and help us identify key features. Decision trees can also be very sensitive to outliers, where as little as one value can result in a completely different tree being generated.\n",
    "\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "> A Decision Tree Classifier was chosen for its simplicity and speed as a classifier. It is relatively easy to build and tune, and provides a quick way to identify the best splitting features and build a classifier. Identifying key splitting features in such a manner can simplify subsequent analysis. It seems as though 'failures' is the best feature to split on, and that we don't recieve benefit from splitting on other features thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model 2: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "K = 5 (default)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.002532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.003933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train   f1_test  train_time  prediction_time\n",
       "0         100  0.855172  0.696296    0.000746         0.001193\n",
       "1         200  0.865248  0.740741    0.000542         0.002532\n",
       "2         300  0.876404  0.733813    0.000783         0.003933"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "K = 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.815047</td>\n",
       "      <td>0.781457</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.002430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.829569</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.005903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train   f1_test  train_time  prediction_time\n",
       "0         100  0.814371  0.786667    0.000622         0.001318\n",
       "1         200  0.815047  0.781457    0.000511         0.002430\n",
       "2         300  0.829569  0.791946    0.000601         0.005903"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"K = 5 (default)\")\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "display(train_and_evaluate_classifier(clf))\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"K = 25\")\n",
    "clf = KNeighborsClassifier(n_neighbors=25)\n",
    "\n",
    "display(train_and_evaluate_classifier(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the theoretical O(n) time & space complexity in terms of input size?\n",
    "> KNN is very cheap to train, but query times can be more expensive. For a query, must identify the closest K neighbors, which involves sorting them by distance (which is also proportional to number of features, `D`). Therefore running time should be a worst case $O(D * n * log(n))$, assuming an algorithm like merge sort is used for sorting. Space complexity should never be more then `O(n)`, as all we need to do is keep track of our samples in memory.\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "> KNN is another relatively simple algorithm that can easily be visualized and interpreted. However, performance can degrade at boundary conditions and when regions are not covered well with samples. Generally, we need a lot of data to cover our feature space; however, it appears that KNN performs well on the test set even when trained on only 100 samples. This could be because KNN is an instance-based algorithm: it is highly dependent on the training data – the first split represents our feature space almost as well as the whole dataset (when evaluated on our test set). We would expect this to vary with different splits. Furthermore, if we are doing a lot of predictions compared to training, it may not be well suited for our task. \n",
    "\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "> The Decision Tree struggled and overfit quickly with `max_depth > 2`. We wanted to identify whether students would cluster in our feature space, as Decision Trees are not quite as good at identifying subtle correlations across many features: they split on the best feature at every iteration. KNN is a simple way to achieve this sort of clustering by performing a distance metric. It appears as though KNN achieves similar testing error on the data as the Decision Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.778523</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.002403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.870488</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.004013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train   f1_test  train_time  prediction_time\n",
       "0         100  0.884615  0.777778    0.003896         0.001073\n",
       "1         200  0.882353  0.778523    0.003995         0.002403\n",
       "2         300  0.870488  0.797297    0.008295         0.004013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "display(train_and_evaluate_classifier(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the theoretical O(n) time & space complexity in terms of input size?\n",
    "> Support Vector Machines have a worst case running time of [O(D * n^3)](http://scikit-learn.org/stable/modules/svm.html#complexity). This makes them much more expensive to train then either of our two previous models. This makes it hard to scale to applications with many samples. The space complexity is generally O(D * n) but can increase drastically based on caching optimizations used for algorithm performance.\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "> Support Vector Machines use quadratic programming to find the optimal hyperplane to seperate two groups. SVMs often perform better then other classification methods because the optimal seperator on training is often superior to other seperators on testing data. Using the kernal trick, SVMs can also implicitely map features to higher dimensions, which can uncover patterns in data other algorithms can not. However, these complexities also result in less interpretable results then KNN or Decision Trees, and a much worse running complexity.\n",
    "\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "> KNN and Decision Trees seemed to perform similarly, I wanted to try SVMs to see if we could uncover some other patterns in the data. It seems like SVMs performed reasonabily well with no hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>train_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.81203</td>\n",
       "      <td>6.313926</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  f1_train  f1_test  train_time  prediction_time\n",
       "0         300  0.810811  0.81203    6.313926         0.000327"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Note: target variable is also transformed from [\"yes\", \"no\"] to [1, 0]\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "params = {\n",
    "    'base_estimator': [DecisionTreeClassifier(max_depth=n) for n in [1, 2, 3, 4, 5, 10]],\n",
    "    'n_estimators': [1, 5, 10, 20, 50, 100],\n",
    "}\n",
    "\n",
    "reg = AdaBoostClassifier()\n",
    "\n",
    "clf = GridSearchCV(reg, param_grid=params, scoring=scorer)\n",
    "\n",
    "display(train_and_evaluate_classifier(clf, [len(X_train)]))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "The final model we chose to deploy in production is **a simple decision tree**. We found that a one stump decision tree has a **slightly better `f1_score`** then KNN or SVM, while having **a lower time complexity**. By running Adaboost with GridSearch, we also saw that there was no added benefit of using multiple decision tree stumps, and that `max_depth=1` is the optimal depth parameter for the tree. Therefore, **Model #1** is our final model. \n",
    "\n",
    "Even though the `f1_test` score for `n_estimators=10` is better then that for `n_estimators=1`, the GridSearch determined that there is a lower training and cross-validation error with `n_estimators=1`. We should never use our testing data to make decisions about tuning our model, therefore we went with the parameters determined by GridSearchCV.\n",
    "\n",
    "Through analysis of the data we determined the best predictive metric for success was the percent of failures. Incorperating other metrics failed to give us any better indicator of students at risk for failure. Students with a failure rate higher then `0.5` should be given extra help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 73.68%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Testing accuracy: {:.2f}%\".format(predict_labels(clf, X_test, y_test, metric_func=accuracy_score)[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "The final model's f1 score is `0.811` on the training data, and `0.812` on the testing data. Our testing and training scores are very similar, which indicates that we have little variance in this model. However, we could be over generalizing, as the data did not seem to provide us much insight beyond what would have been a logical decision (splitting on failure rate). Perhaps with more data we could better cover our feature space, and determine some correlation between other features.\n",
    "\n",
    "While `f1_score` maximizes both precision and recall, we notice our accuracy is still pretty low, at around `74%`. If there is a low economic cost of flagging students as at risk to fail, we may want to focus on maximizing our precision. This would result in more false positives, but also more true positives which could be more important to us. This is something for the committee to decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"258pt\" height=\"158pt\"\n",
       " viewBox=\"0.00 0.00 258.02 158.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 154)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-154 254.02,-154 254.02,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.537255\" stroke=\"black\" d=\"M174.87,-150C174.87,-150 79.0434,-150 79.0434,-150 73.0434,-150 67.0434,-144 67.0434,-138 67.0434,-138 67.0434,-98 67.0434,-98 67.0434,-92 73.0434,-86 79.0434,-86 79.0434,-86 174.87,-86 174.87,-86 180.87,-86 186.87,-92 186.87,-98 186.87,-98 186.87,-138 186.87,-138 186.87,-144 180.87,-150 174.87,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"86.9287\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">failures ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"86.6724\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4328</text>\n",
       "<text text-anchor=\"start\" x=\"81.2344\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 300</text>\n",
       "<text text-anchor=\"start\" x=\"75\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [95, 205]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.670588\" stroke=\"black\" d=\"M107.87,-50C107.87,-50 12.0434,-50 12.0434,-50 6.04343,-50 0.0434256,-44 0.0434256,-38 0.0434256,-38 0.0434256,-12 0.0434256,-12 0.0434256,-6 6.04343,-0 12.0434,-0 12.0434,-0 107.87,-0 107.87,-0 113.87,-0 119.87,-6 119.87,-12 119.87,-12 119.87,-38 119.87,-38 119.87,-44 113.87,-50 107.87,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"19.6724\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.3718</text>\n",
       "<text text-anchor=\"start\" x=\"14.2344\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 239</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [59, 180]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.086,-85.9375C97.5364,-77.0413 90.3816,-67.3236 83.7761,-58.352\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"86.3965,-56.0077 77.649,-50.0301 80.7595,-60.158 86.3965,-56.0077\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.8946\" y=\"-70.5466\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.305882\" stroke=\"black\" d=\"M238.084,-50C238.084,-50 149.83,-50 149.83,-50 143.83,-50 137.83,-44 137.83,-38 137.83,-38 137.83,-12 137.83,-12 137.83,-6 143.83,-0 149.83,-0 149.83,-0 238.084,-0 238.084,-0 244.084,-0 250.084,-6 250.084,-12 250.084,-12 250.084,-38 250.084,-38 250.084,-44 244.084,-50 238.084,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"153.672\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4837</text>\n",
       "<text text-anchor=\"start\" x=\"152.127\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 61</text>\n",
       "<text text-anchor=\"start\" x=\"145.893\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [36, 25]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M149.827,-85.9375C156.377,-77.0413 163.531,-67.3236 170.137,-58.352\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.154,-60.158 176.264,-50.0301 167.517,-56.0077 173.154,-60.158\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.018\" y=\"-70.5466\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x10da0dac8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Code to build decision tree visualization\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "import pydotplus as pydot\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "train_classifier(clf, X_train, y_train)\n",
    "\n",
    "fname = 'my_tree.dot'\n",
    "export_graphviz(clf, out_file=fname,  \n",
    "                feature_names=X_all.columns,  \n",
    "                filled=True, rounded=True,  \n",
    "                special_characters=True)\n",
    "\n",
    "with open(fname) as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "### Training\n",
    "\n",
    "Decision Trees are a recursive algorithm that build a tree by making binary (yes or no) decisions to make predictions on data. At each iteration, the algorithm looks at the feature which best (most accurate prediction) splits the training data. Successive splits are independent of one another. For example, the decision tree below first looks at the rate of failures of a student, as this was the feature with the best prediction rate. Out of the group with a rate of failures `<= 0.5` the best splitting metric turned out to be `schoolsup` (suplementary school work). Several parameters can be tuned to prevent the tree from overfitting, which would cause bad results on testing data. This tree is then used for prediction.\n",
    "\n",
    "### Prediction\n",
    "For students with a failure rate `<= 0.5`, it looks at students with `schoolsup > 0.5` and flags them as at risk for failure. `schoolsup` (supplementary school work) was indicated as the best splitting feature after a failure rate of ` <= 0.5`. For students with a failure rate `> 0.5` it looks at students with absenses `> 17.5`, and flags them as at risk to fail. Students with `<= 17.5` absenses are not flagged.\n",
    "\n",
    "## Our Final Tree\n",
    "We found that the optimal tree only had one node: an individaul's failure rates. It flags them if they are greater than `0.5`. This was the best indicator of failure we found by looking at the dataset. While adding more nodes reduced our training error, it did not reduce our testing error: there seems to be more successive correlation between other features and student success. See the visualization below to see the result of the decision tree trained on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Graph\n",
    "For illustration only, this was not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"505pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 505.23 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-254 501.234,-254 501.234,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.537255\" stroke=\"black\" d=\"M307.87,-250C307.87,-250 212.043,-250 212.043,-250 206.043,-250 200.043,-244 200.043,-238 200.043,-238 200.043,-198 200.043,-198 200.043,-192 206.043,-186 212.043,-186 212.043,-186 307.87,-186 307.87,-186 313.87,-186 319.87,-192 319.87,-198 319.87,-198 319.87,-238 319.87,-238 319.87,-244 313.87,-250 307.87,-250\"/>\n",
       "<text text-anchor=\"start\" x=\"219.929\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">failures ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"219.672\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4328</text>\n",
       "<text text-anchor=\"start\" x=\"214.234\" y=\"-206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 300</text>\n",
       "<text text-anchor=\"start\" x=\"208\" y=\"-192.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [95, 205]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.670588\" stroke=\"black\" d=\"M239.87,-150C239.87,-150 144.043,-150 144.043,-150 138.043,-150 132.043,-144 132.043,-138 132.043,-138 132.043,-98 132.043,-98 132.043,-92 138.043,-86 144.043,-86 144.043,-86 239.87,-86 239.87,-86 245.87,-86 251.87,-92 251.87,-98 251.87,-98 251.87,-138 251.87,-138 251.87,-144 245.87,-150 239.87,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"142.974\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">schoolsup ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"151.672\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.3718</text>\n",
       "<text text-anchor=\"start\" x=\"146.234\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 239</text>\n",
       "<text text-anchor=\"start\" x=\"140\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [59, 180]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.441,-185.992C232.364,-177.234 225.668,-167.585 219.298,-158.404\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.142,-156.363 213.566,-150.142 216.391,-160.354 222.142,-156.363\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.12\" y=\"-170.544\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.305882\" stroke=\"black\" d=\"M376.099,-150C376.099,-150 281.814,-150 281.814,-150 275.814,-150 269.814,-144 269.814,-138 269.814,-138 269.814,-98 269.814,-98 269.814,-92 275.814,-86 281.814,-86 281.814,-86 376.099,-86 376.099,-86 382.099,-86 388.099,-92 388.099,-98 388.099,-98 388.099,-138 388.099,-138 388.099,-144 382.099,-150 376.099,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"277.636\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">absences ≤ 17.5</text>\n",
       "<text text-anchor=\"start\" x=\"288.672\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4837</text>\n",
       "<text text-anchor=\"start\" x=\"287.127\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 61</text>\n",
       "<text text-anchor=\"start\" x=\"280.893\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [36, 25]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.789,-185.992C287.955,-177.234 294.749,-167.585 301.213,-158.404\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.134,-160.334 307.03,-150.142 298.411,-156.304 304.134,-160.334\"/>\n",
       "<text text-anchor=\"middle\" x=\"311.307\" y=\"-170.574\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.721569\" stroke=\"black\" d=\"M107.87,-50C107.87,-50 12.0434,-50 12.0434,-50 6.04343,-50 0.0434256,-44 0.0434256,-38 0.0434256,-38 0.0434256,-12 0.0434256,-12 0.0434256,-6 6.04343,-0 12.0434,-0 12.0434,-0 107.87,-0 107.87,-0 113.87,-0 119.87,-6 119.87,-12 119.87,-12 119.87,-38 119.87,-38 119.87,-44 113.87,-50 107.87,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"19.6724\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.3398</text>\n",
       "<text text-anchor=\"start\" x=\"14.2344\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 212</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [46, 166]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M146.899,-85.9375C132.797,-76.2158 117.272,-65.5132 103.292,-55.8752\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"105.033,-52.8244 94.8134,-50.0301 101.06,-58.5876 105.033,-52.8244\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.070588\" stroke=\"black\" d=\"M238.084,-50C238.084,-50 149.83,-50 149.83,-50 143.83,-50 137.83,-44 137.83,-38 137.83,-38 137.83,-12 137.83,-12 137.83,-6 143.83,-0 149.83,-0 149.83,-0 238.084,-0 238.084,-0 244.084,-0 250.084,-6 250.084,-12 250.084,-12 250.084,-38 250.084,-38 250.084,-44 244.084,-50 238.084,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"153.672\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4993</text>\n",
       "<text text-anchor=\"start\" x=\"152.127\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 27</text>\n",
       "<text text-anchor=\"start\" x=\"145.893\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [13, 14]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.639,-85.9375C192.821,-77.6833 193.018,-68.7219 193.203,-60.3053\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.708,-60.1046 193.428,-50.0301 189.709,-59.9507 196.708,-60.1046\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.470588\" stroke=\"black\" d=\"M371.084,-50C371.084,-50 282.83,-50 282.83,-50 276.83,-50 270.83,-44 270.83,-38 270.83,-38 270.83,-12 270.83,-12 270.83,-6 276.83,-0 282.83,-0 282.83,-0 371.084,-0 371.084,-0 377.084,-0 383.084,-6 383.084,-12 383.084,-12 383.084,-38 383.084,-38 383.084,-44 377.084,-50 371.084,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"286.672\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4522</text>\n",
       "<text text-anchor=\"start\" x=\"285.127\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 55</text>\n",
       "<text text-anchor=\"start\" x=\"278.893\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [36, 19]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328.274,-85.9375C328.092,-77.6833 327.895,-68.7219 327.711,-60.3053\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.204,-59.9507 327.485,-50.0301 324.205,-60.1046 331.204,-59.9507\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M485.012,-50C485.012,-50 412.901,-50 412.901,-50 406.901,-50 400.901,-44 400.901,-38 400.901,-38 400.901,-12 400.901,-12 400.901,-6 406.901,-0 412.901,-0 412.901,-0 485.012,-0 485.012,-0 491.012,-0 497.012,-6 497.012,-12 497.012,-12 497.012,-38 497.012,-38 497.012,-44 491.012,-50 485.012,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"420.352\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"411.021\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"408.679\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 6]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.918,-85.9375C382.617,-76.3076 396.585,-65.7151 409.201,-56.1483\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.415,-58.8613 417.269,-50.0301 407.186,-53.2837 411.415,-58.8613\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x10e1153c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphviz.Source(dot_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
